{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8655403",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting rdkit\n",
      "  Downloading rdkit-2024.9.6-cp39-cp39-win_amd64.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\gagno\\anaconda3\\envs\\fresh_env\\lib\\site-packages (from rdkit) (1.24.3)\n",
      "Requirement already satisfied: Pillow in c:\\users\\gagno\\anaconda3\\envs\\fresh_env\\lib\\site-packages (from rdkit) (9.4.0)\n",
      "Downloading rdkit-2024.9.6-cp39-cp39-win_amd64.whl (22.5 MB)\n",
      "   ---------------------------------------- 0.0/22.5 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 2.1/22.5 MB 14.7 MB/s eta 0:00:02\n",
      "   --------- ------------------------------ 5.5/22.5 MB 14.0 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 8.1/22.5 MB 13.6 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 10.5/22.5 MB 13.3 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 12.6/22.5 MB 13.1 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 14.9/22.5 MB 12.2 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 17.8/22.5 MB 12.5 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 19.9/22.5 MB 12.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 22.5/22.5 MB 12.4 MB/s eta 0:00:00\n",
      "Installing collected packages: rdkit\n",
      "Successfully installed rdkit-2024.9.6\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.6.1-cp39-cp39-win_amd64.whl.metadata (15 kB)\n",
      "Requirement already satisfied: numpy>=1.19.5 in c:\\users\\gagno\\anaconda3\\envs\\fresh_env\\lib\\site-packages (from scikit-learn) (1.24.3)\n",
      "Collecting scipy>=1.6.0 (from scikit-learn)\n",
      "  Downloading scipy-1.13.1-cp39-cp39-win_amd64.whl.metadata (60 kB)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Using cached joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Downloading threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Downloading scikit_learn-1.6.1-cp39-cp39-win_amd64.whl (11.2 MB)\n",
      "   ---------------------------------------- 0.0/11.2 MB ? eta -:--:--\n",
      "   ------------------- -------------------- 5.5/11.2 MB 30.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 11.2/11.2 MB 30.3 MB/s eta 0:00:00\n",
      "Using cached joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Downloading scipy-1.13.1-cp39-cp39-win_amd64.whl (46.2 MB)\n",
      "   ---------------------------------------- 0.0/46.2 MB ? eta -:--:--\n",
      "   ----- ---------------------------------- 6.8/46.2 MB 35.0 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 14.2/46.2 MB 34.2 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 21.0/46.2 MB 34.9 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 28.3/46.2 MB 34.5 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 35.4/46.2 MB 34.6 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 42.5/46.2 MB 34.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 46.2/46.2 MB 32.3 MB/s eta 0:00:00\n",
      "Downloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, scipy, joblib, scikit-learn\n",
      "Successfully installed joblib-1.4.2 scikit-learn-1.6.1 scipy-1.13.1 threadpoolctl-3.6.0\n",
      "Collecting lightgbm\n",
      "  Downloading lightgbm-4.6.0-py3-none-win_amd64.whl.metadata (17 kB)\n",
      "Requirement already satisfied: numpy>=1.17.0 in c:\\users\\gagno\\anaconda3\\envs\\fresh_env\\lib\\site-packages (from lightgbm) (1.24.3)\n",
      "Requirement already satisfied: scipy in c:\\users\\gagno\\anaconda3\\envs\\fresh_env\\lib\\site-packages (from lightgbm) (1.13.1)\n",
      "Downloading lightgbm-4.6.0-py3-none-win_amd64.whl (1.5 MB)\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.5/1.5 MB 25.3 MB/s eta 0:00:00\n",
      "Installing collected packages: lightgbm\n",
      "Successfully installed lightgbm-4.6.0\n",
      "Collecting xgboost\n",
      "  Downloading xgboost-2.1.4-py3-none-win_amd64.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\gagno\\anaconda3\\envs\\fresh_env\\lib\\site-packages (from xgboost) (1.24.3)\n",
      "Requirement already satisfied: scipy in c:\\users\\gagno\\anaconda3\\envs\\fresh_env\\lib\\site-packages (from xgboost) (1.13.1)\n",
      "Downloading xgboost-2.1.4-py3-none-win_amd64.whl (124.9 MB)\n",
      "   ---------------------------------------- 0.0/124.9 MB ? eta -:--:--\n",
      "   - -------------------------------------- 6.0/124.9 MB 33.5 MB/s eta 0:00:04\n",
      "   ---- ----------------------------------- 13.4/124.9 MB 33.5 MB/s eta 0:00:04\n",
      "   ------ --------------------------------- 20.4/124.9 MB 34.0 MB/s eta 0:00:04\n",
      "   -------- ------------------------------- 27.5/124.9 MB 34.2 MB/s eta 0:00:03\n",
      "   ---------- ----------------------------- 34.3/124.9 MB 34.1 MB/s eta 0:00:03\n",
      "   ------------- -------------------------- 41.7/124.9 MB 34.4 MB/s eta 0:00:03\n",
      "   --------------- ------------------------ 48.8/124.9 MB 34.5 MB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 56.1/124.9 MB 34.4 MB/s eta 0:00:03\n",
      "   -------------------- ------------------- 63.7/124.9 MB 34.4 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 70.8/124.9 MB 34.4 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 78.1/124.9 MB 34.4 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 85.7/124.9 MB 34.4 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 93.1/124.9 MB 34.3 MB/s eta 0:00:01\n",
      "   ------------------------------- ------- 100.1/124.9 MB 34.4 MB/s eta 0:00:01\n",
      "   --------------------------------- ----- 107.2/124.9 MB 34.4 MB/s eta 0:00:01\n",
      "   ----------------------------------- --- 114.3/124.9 MB 34.4 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 121.4/124.9 MB 34.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  124.8/124.9 MB 34.3 MB/s eta 0:00:01\n",
      "   --------------------------------------- 124.9/124.9 MB 32.3 MB/s eta 0:00:00\n",
      "Installing collected packages: xgboost\n",
      "Successfully installed xgboost-2.1.4\n",
      "Requirement already satisfied: joblib in c:\\users\\gagno\\anaconda3\\envs\\fresh_env\\lib\\site-packages (1.4.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install rdkit\n",
    "!pip install scikit-learn\n",
    "!pip install lightgbm\n",
    "!pip install xgboost\n",
    "!pip install joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc0d7d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit import Chem\n",
    "from rdkit.Chem import Draw\n",
    "from rdkit.Chem.Draw import IPythonConsole\n",
    "from rdkit.Chem import Descriptors, rdMolDescriptors\n",
    "from rdkit.Chem import AllChem\n",
    "from rdkit import DataStructs\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import lightgbm as lgb\n",
    "from lightgbm import early_stopping, log_evaluation\n",
    "from xgboost import XGBRegressor\n",
    "import joblib\n",
    "from rdkit.Chem import Descriptors\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, KFold\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import r2_score, mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f8b00aaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<rdkit.Chem.rdchem.Mol at 0x1d50f6fbd60>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mol = Chem.MolFromSmiles('CCCC')\n",
    "mol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f4acc2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# RDKit for SMILES parsing\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "\n",
    "# For train-test split, metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# (Optional) Feature scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# XGBoost\n",
    "# pip install xgboost\n",
    "import xgboost as xgb\n",
    "\n",
    "def smiles_to_morgan_fp(smi, radius=2, n_bits=256):\n",
    "    \"\"\"\n",
    "    Convert a SMILES string to a Morgan fingerprint (ECFP) vector of length n_bits.\n",
    "    Returns a NumPy array of shape (n_bits,) or None if the SMILES is invalid.\n",
    "\n",
    "    We use 256 bits (instead of 1024) to reduce dimensionality for this small dataset.\n",
    "    \"\"\"\n",
    "    mol = Chem.MolFromSmiles(smi)\n",
    "    if mol is None:\n",
    "        return None\n",
    "    fp = AllChem.GetMorganFingerprintAsBitVect(mol, radius, nBits=n_bits)\n",
    "    arr = np.zeros((n_bits,), dtype=int)\n",
    "    for i in range(n_bits):\n",
    "        if fp.GetBit(i):\n",
    "            arr[i] = 1\n",
    "    return arr\n",
    "\n",
    "def main():\n",
    "    # 1. Load your CSV dataset (adjust filename/column names as needed)\n",
    "    file_path = 'C:/Users/gagno/Downloads/molecule_dataset.csv'\n",
    "    df = pd.read_csv(file_path)\n",
    "    print(df.head(18))  # CSV should include 'SMILES', 'enhancement_factor', and 16 descriptor columns\n",
    "    \n",
    "    # 2. Basic cleaning: handle infinities and ensure 'Enhancement Factor' is numeric\n",
    "    df = df.replace([np.inf, -np.inf], np.nan)\n",
    "    df[\"Enhancement Factor\"] = pd.to_numeric(df[\"Enhancement Factor\"], errors='coerce')\n",
    "    \n",
    "    # Drop rows with missing values in critical columns\n",
    "    df.dropna(subset=[\"Smiles\", \"Enhancement Factor\"], inplace=True)\n",
    "    \n",
    "    # 3. Convert SMILES to numeric features (Morgan fingerprints)\n",
    "    smiles_list = df[\"Smiles\"].astype(str).tolist()\n",
    "    enhancement_factors = df[\"Enhancement Factor\"].values\n",
    "    \n",
    "    X_fps = []\n",
    "    valid_y = []\n",
    "    for smi, val in zip(smiles_list, enhancement_factors):\n",
    "        fp = smiles_to_morgan_fp(smi, radius=2, n_bits=256)\n",
    "        if fp is not None:\n",
    "            X_fps.append(fp)\n",
    "            valid_y.append(val)\n",
    "    X = np.array(X_fps, dtype=float)\n",
    "    y = np.array(valid_y, dtype=float)\n",
    "    \n",
    "    # 4. Train-test split\n",
    "    #    We'll do 80-20, but with only 16 rows, cross-validation is recommended for a better estimate.\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42\n",
    "    )\n",
    "    \n",
    "    # 5. (Optional) Feature scaling\n",
    "    #    XGBoost often handles unscaled data well, but scaling can help if you have extra numeric descriptors.\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled  = scaler.transform(X_test)\n",
    "    \n",
    "    # 6. Define and train an XGBoost regressor\n",
    "    #    Adjust n_estimators, max_depth, learning_rate, etc. via cross-validation if possible.\n",
    "    model = xgb.XGBRegressor(\n",
    "        n_estimators=100,\n",
    "        max_depth=3,\n",
    "        learning_rate=0.1,\n",
    "        random_state=42,\n",
    "        verbosity=0  # suppress training logs\n",
    "    )\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # 7. Evaluate on the test set\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    \n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    # Adjusted R² = 1 - (1 - R²)*(n - 1)/(n - p - 1)\n",
    "    n = len(y_test)\n",
    "    p = X_test_scaled.shape[1]  # e.g., 256 if not reduced further\n",
    "    if n > p + 1:\n",
    "        adjusted_r2 = 1 - (1 - r2) * (n - 1) / (n - p - 1)\n",
    "    else:\n",
    "        adjusted_r2 = float('nan')  # Not well-defined if p >= n-1\n",
    "    \n",
    "    print(\"\\n=== XGBoost Model (Test Set) ===\")\n",
    "    print(\"n_estimators=100, max_depth=3, learning_rate=0.1\")\n",
    "    print(f\"MSE  : {mse:.4f}\")\n",
    "    print(f\"RMSE : {rmse:.4f}\")\n",
    "    print(f\"MAE  : {mae:.4f}\")\n",
    "    print(f\"R²   : {r2:.4f}\")\n",
    "    print(f\"Adjusted R²: {adjusted_r2:.4f}\")\n",
    "\n",
    "    # 8. (Optional) Retrain a final model on the entire dataset for new SMILES predictions\n",
    "    final_model = xgb.XGBRegressor(\n",
    "        n_estimators=100,\n",
    "        max_depth=3,\n",
    "        learning_rate=0.1,\n",
    "        random_state=42,\n",
    "        verbosity=0\n",
    "    )\n",
    "    # Scale the entire dataset\n",
    "    X_scaled = scaler.fit_transform(X)  # re-fit on all data\n",
    "    final_model.fit(X_scaled, y)\n",
    "    \n",
    "    # 9. Interactive input for new SMILES outside the dataset\n",
    "    print(\"\\n=== Predict Enhancement Factor for a new SMILES ===\")\n",
    "    while True:\n",
    "        user_input = input(\"Enter a SMILES string (or type 'quit' to exit): \").strip()\n",
    "        if user_input.lower() == 'quit':\n",
    "            print(\"Exiting.\")\n",
    "            break\n",
    "        \n",
    "        new_fp = smiles_to_morgan_fp(user_input, radius=2, n_bits=256)\n",
    "        if new_fp is None:\n",
    "            print(\"Invalid SMILES entered. Please try again.\\n\")\n",
    "            continue\n",
    "        \n",
    "        # Scale the new fingerprint\n",
    "        new_fp_scaled = scaler.transform([new_fp])\n",
    "        pred_val = final_model.predict(new_fp_scaled)[0]\n",
    "        print(f\"Predicted Enhancement Factor: {pred_val:.4f}\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4350555",
   "metadata": {},
   "source": [
    "Here is an improved version of model\n",
    "\n",
    "-\n",
    "-\n",
    "-\n",
    "-\n",
    "-\n",
    "-\n",
    "-\n",
    "-\n",
    "-\n",
    "-\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a977f155",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ID                         Name  \\\n",
      "0     1         Ibuprofen / HP-B-CD   \n",
      "1     2      Dexamethasone / B - CD   \n",
      "2     3         Piroxicam / HP-B-CD   \n",
      "3     4    Hydrocortisone / HP-B-CD   \n",
      "4     5       Albendazole / RM-B-CD   \n",
      "5     6            Valtarsan / B-CD   \n",
      "6     7        Nifedipine / HP-B-CD   \n",
      "7     8        Nimodipine / HP-B-CD   \n",
      "8     9     Carbamazepine / HP-B-CD   \n",
      "9    10      Itraconazole / HP-B-CD   \n",
      "10   11       Simvastatin / HP-B-CD   \n",
      "11   12          Curcumin / HP-B-CD   \n",
      "12   13  Indomethacin / HP- B - CD    \n",
      "13   14     Naproxen / HP - B - CD    \n",
      "14   15        Diclofenac / HP-B-CD   \n",
      "15   16   Resveratrol / HP-Gamma CD   \n",
      "16   17              Amphotericin B   \n",
      "17   18                 Budesonide    \n",
      "\n",
      "                                               Smiles MolWeight  LogP  \\\n",
      "0                          CC(C)Cc1ccc(cc1)C(C)C(=O)O   1606.29  3.84   \n",
      "1   CC(C)C1C(=O)CC2C(C1O)(CC(C1=CC(=O)C=C2C1(C)O)F...   1527.47  1.93   \n",
      "2                     CN1NS(=O)(=O)C2=C(O)C=CN=C2C1=O   1466.35   2.2   \n",
      "3              CC(=O)C1(CCC2C1(CCC1C(C2CC(C1O)O)O)C)O   1762.46  1.79   \n",
      "4                      CCCSC1=CC=C2C=C1N=C(C(=O)OC)N2   250.323  3.22   \n",
      "5   CC(C)C[C@H](C(=O)NCc1ccc(cc1)c1ccccc1C(=N)N=N)...    1570.5  3.68   \n",
      "6   CCOC(=O)C1=C(C)NC(=C(C1c1ccccc1[N+](=O)[O-])C)...      1846  2.49   \n",
      "7   COCCOC(=O)C1=C(C)NC(=C(C1c1ccccc1[N+](=O)[O-])...    418.44  3.41   \n",
      "8                        O=C1Nc2ccccc2C2=CC=CC=C2C1=O  1371.25    2.1   \n",
      "9   CC1=NC(C)=NC(OCC2CCC(OC3=NC(N4CCN(C)CC4)=NC(C4...    1840.6  5.48   \n",
      "10  CC(C)CC(CC=C)C(=O)OC[C@@H]1CC[C@@H](CC1)C(=O)O...   1553.55  4.51   \n",
      "11      COc1ccc(cc1)/C=C/C(=O)CC(=O)/C=C/c1ccc(cc1)OC   1503.36  3.62   \n",
      "12      COc1ccc2c(c1)n(c(c2C(=O)O)C(=O)c1ccc(cc1)Cl)C    1492.8  4.46   \n",
      "13                       COc1cc2ccc(C(C)C(=O)O)cc2cc1   1365.24  3.39   \n",
      "14                   OC(=O)Cc1cccc(c1)Nc1c(cccc1Cl)Cl   1431.15  4.75   \n",
      "15               OC1=CC=CC(/C=C/C2=CC(O)=CC(O)=C2)=C1  1728.247  2.57   \n",
      "16  CC(C)CCCC(C)CCCC(C)C(C)CCC/C=C\\C/C=C\\C/C=C\\C/C...         -     -   \n",
      "17  CC(=CC(=O)C12CCC(C)CC1CCC1C2CCC2OC(C)(C)CC1(C)...         -     -   \n",
      "\n",
      "       LogS (CD)  HPCalculated  Descriptors   \\\n",
      "0           -3.5           NaN           NaN   \n",
      "1    2.979852707           NaN           NaN   \n",
      "2    4.166237644           NaN           NaN   \n",
      "3    1.831145921           NaN           NaN   \n",
      "4      0.9213795           NaN           NaN   \n",
      "5    2.920818754           NaN           NaN   \n",
      "6    3.915983678           NaN           NaN   \n",
      "7    3.881270504           NaN           NaN   \n",
      "8   -2.331615782           NaN           NaN   \n",
      "9    4.833595654           NaN           NaN   \n",
      "10    3.93605273           NaN           NaN   \n",
      "11    4.08504452           NaN           NaN   \n",
      "12   4.229518954           NaN           NaN   \n",
      "13   1.754487332           NaN           NaN   \n",
      "14   4.803502637           NaN           NaN   \n",
      "15   3.196213127           NaN           NaN   \n",
      "16             -           NaN           NaN   \n",
      "17             -           NaN           NaN   \n",
      "\n",
      "    CD-complex solubility (mg/ml; unless stated otherwise)  \\\n",
      "0                                                 NaN        \n",
      "1                                            1.600000        \n",
      "2                                            0.100000        \n",
      "3                                           26.000000        \n",
      "4                                           30.000000        \n",
      "5                                            1.884600        \n",
      "6                                            0.224000        \n",
      "7                                            0.055000        \n",
      "8                                            6.390000        \n",
      "9                                            0.027000        \n",
      "10                                           0.180000        \n",
      "11                                           0.123600        \n",
      "12                                           0.088000        \n",
      "13                                          24.028224        \n",
      "14                                           0.022500        \n",
      "15                                           1.100000        \n",
      "16                                           0.150000        \n",
      "17                                           0.845000        \n",
      "\n",
      "    Intrinistic solubility (mg/ml; unless stated otherwise  Enhancement Factor  \n",
      "0                                                 NaN                30.000000  \n",
      "1                                            0.160000                10.000000  \n",
      "2                                            0.023000                 4.347826  \n",
      "3                                            0.280000                71.400000  \n",
      "4                                            0.010000               600.000000  \n",
      "5                                            0.003192               590.783000  \n",
      "6                                            0.005000                44.800000  \n",
      "7                                            0.002500                22.000000  \n",
      "8                                            0.140000                45.600000  \n",
      "9                                            0.000001             27000.000000  \n",
      "10                                           0.030000                 6.000000  \n",
      "11                                           0.000600               206.000000  \n",
      "12                                           0.008800                10.000000  \n",
      "13                                           0.115131               208.704900  \n",
      "14                                           0.003000                 7.500000  \n",
      "15                                           0.030000                36.666667  \n",
      "16                                           0.001000               150.000000  \n",
      "17                                           0.022000                38.409091  \n",
      "\n",
      "=== XGBoost Model (Test Set) ===\n",
      "Log-transform of Enhancement Factor + 256-bit fingerprints\n",
      "n_estimators=100, max_depth=3, learning_rate=0.1\n",
      "MSE  : 151159.4567\n",
      "RMSE : 388.7923\n",
      "MAE  : 195.5190\n",
      "R²   : -0.2341\n",
      "Adjusted R²: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[02:07:36] DEPRECATION WARNING: please use MorganGenerator\n",
      "[02:07:36] DEPRECATION WARNING: please use MorganGenerator\n",
      "[02:07:36] DEPRECATION WARNING: please use MorganGenerator\n",
      "[02:07:36] DEPRECATION WARNING: please use MorganGenerator\n",
      "[02:07:36] DEPRECATION WARNING: please use MorganGenerator\n",
      "[02:07:36] DEPRECATION WARNING: please use MorganGenerator\n",
      "[02:07:36] DEPRECATION WARNING: please use MorganGenerator\n",
      "[02:07:36] DEPRECATION WARNING: please use MorganGenerator\n",
      "[02:07:36] DEPRECATION WARNING: please use MorganGenerator\n",
      "[02:07:36] DEPRECATION WARNING: please use MorganGenerator\n",
      "[02:07:36] DEPRECATION WARNING: please use MorganGenerator\n",
      "[02:07:36] DEPRECATION WARNING: please use MorganGenerator\n",
      "[02:07:36] DEPRECATION WARNING: please use MorganGenerator\n",
      "[02:07:36] DEPRECATION WARNING: please use MorganGenerator\n",
      "[02:07:36] DEPRECATION WARNING: please use MorganGenerator\n",
      "[02:07:36] DEPRECATION WARNING: please use MorganGenerator\n",
      "[02:07:36] DEPRECATION WARNING: please use MorganGenerator\n",
      "[02:07:36] DEPRECATION WARNING: please use MorganGenerator\n",
      "[02:07:36] DEPRECATION WARNING: please use MorganGenerator\n",
      "[02:07:36] DEPRECATION WARNING: please use MorganGenerator\n",
      "[02:07:36] DEPRECATION WARNING: please use MorganGenerator\n",
      "[02:07:36] DEPRECATION WARNING: please use MorganGenerator\n",
      "[02:07:36] DEPRECATION WARNING: please use MorganGenerator\n",
      "[02:07:36] DEPRECATION WARNING: please use MorganGenerator\n",
      "[02:07:36] DEPRECATION WARNING: please use MorganGenerator\n",
      "[02:07:36] DEPRECATION WARNING: please use MorganGenerator\n",
      "[02:07:36] DEPRECATION WARNING: please use MorganGenerator\n",
      "[02:07:36] DEPRECATION WARNING: please use MorganGenerator\n",
      "[02:07:36] DEPRECATION WARNING: please use MorganGenerator\n",
      "[02:07:36] DEPRECATION WARNING: please use MorganGenerator\n",
      "[02:07:36] DEPRECATION WARNING: please use MorganGenerator\n",
      "[02:07:36] DEPRECATION WARNING: please use MorganGenerator\n",
      "[02:07:36] DEPRECATION WARNING: please use MorganGenerator\n",
      "[02:07:36] DEPRECATION WARNING: please use MorganGenerator\n",
      "[02:07:36] DEPRECATION WARNING: please use MorganGenerator\n",
      "[02:07:36] DEPRECATION WARNING: please use MorganGenerator\n",
      "[02:07:36] DEPRECATION WARNING: please use MorganGenerator\n",
      "[02:07:36] DEPRECATION WARNING: please use MorganGenerator\n",
      "[02:07:36] DEPRECATION WARNING: please use MorganGenerator\n",
      "[02:07:36] DEPRECATION WARNING: please use MorganGenerator\n",
      "[02:07:36] DEPRECATION WARNING: please use MorganGenerator\n",
      "[02:07:36] DEPRECATION WARNING: please use MorganGenerator\n",
      "[02:07:36] DEPRECATION WARNING: please use MorganGenerator\n",
      "[02:07:36] DEPRECATION WARNING: please use MorganGenerator\n",
      "[02:07:36] DEPRECATION WARNING: please use MorganGenerator\n",
      "[02:07:36] DEPRECATION WARNING: please use MorganGenerator\n",
      "[02:07:36] DEPRECATION WARNING: please use MorganGenerator\n",
      "[02:07:36] DEPRECATION WARNING: please use MorganGenerator\n",
      "[02:07:36] DEPRECATION WARNING: please use MorganGenerator\n",
      "[02:07:36] DEPRECATION WARNING: please use MorganGenerator\n",
      "[02:07:36] DEPRECATION WARNING: please use MorganGenerator\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Predict Enhancement Factor for a new SMILES ===\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter a SMILES string (or type 'quit' to exit):  quit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exiting.\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# RDKit for SMILES parsing\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "\n",
    "# For train-test split, metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# (Optional) Feature scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# XGBoost\n",
    "# pip install xgboost\n",
    "import xgboost as xgb\n",
    "\n",
    "def smiles_to_morgan_fp(smi, radius=2, n_bits=256):\n",
    "    \"\"\"\n",
    "    Convert a SMILES string to a Morgan fingerprint (ECFP) vector of length n_bits.\n",
    "    Returns a NumPy array of shape (n_bits,) or None if the SMILES is invalid.\n",
    "\n",
    "    We use 256 bits (instead of 1024) to reduce dimensionality for a small dataset.\n",
    "    \"\"\"\n",
    "    mol = Chem.MolFromSmiles(smi)\n",
    "    if mol is None:\n",
    "        return None\n",
    "    fp = AllChem.GetMorganFingerprintAsBitVect(mol, radius, nBits=n_bits)\n",
    "    arr = np.zeros((n_bits,), dtype=int)\n",
    "    for i in range(n_bits):\n",
    "        if fp.GetBit(i):\n",
    "            arr[i] = 1\n",
    "    return arr\n",
    "\n",
    "def main():\n",
    "    # 1. Load your CSV dataset\n",
    "    file_path = 'C:/Users/gagno/Downloads/molecule_dataset.csv'\n",
    "    df = pd.read_csv(file_path)\n",
    "    print(df.head(18))  # Quick peek at data\n",
    "    \n",
    "    # 2. Basic cleaning\n",
    "    df = df.replace([np.inf, -np.inf], np.nan)\n",
    "    df[\"Enhancement Factor\"] = pd.to_numeric(df[\"Enhancement Factor\"], errors='coerce')\n",
    "    df.dropna(subset=[\"Smiles\", \"Enhancement Factor\"], inplace=True)\n",
    "    \n",
    "    # 3. Convert SMILES to numeric features (256-bit Morgan fingerprints)\n",
    "    smiles_list = df[\"Smiles\"].astype(str).tolist()\n",
    "    enhancement_factors = df[\"Enhancement Factor\"].values\n",
    "    \n",
    "    X_fps = []\n",
    "    valid_y = []\n",
    "    for smi, val in zip(smiles_list, enhancement_factors):\n",
    "        fp = smiles_to_morgan_fp(smi, radius=2, n_bits=256)\n",
    "        if fp is not None:\n",
    "            X_fps.append(fp)\n",
    "            valid_y.append(val)\n",
    "    X = np.array(X_fps, dtype=float)\n",
    "    y = np.array(valid_y, dtype=float)\n",
    "    \n",
    "    # --- NEW: Log-Transform the Enhancement Factor ---\n",
    "    # If EF is always > 0, np.log1p helps handle large numeric ranges or outliers.\n",
    "    y_log = np.log1p(y)  # log(1 + EF)\n",
    "    \n",
    "    # 4. Train-test split (80-20)\n",
    "    X_train, X_test, y_train_log, y_test_log = train_test_split(\n",
    "        X, y_log, test_size=0.2, random_state=42\n",
    "    )\n",
    "    \n",
    "    # 5. (Optional) Feature scaling\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled  = scaler.transform(X_test)\n",
    "    \n",
    "    # 6. Define and train an XGBoost regressor\n",
    "    #    We'll keep the same hyperparameters but train on y_train_log\n",
    "    model = xgb.XGBRegressor(\n",
    "        n_estimators=100,\n",
    "        max_depth=3,\n",
    "        learning_rate=0.1,\n",
    "        random_state=42,\n",
    "        verbosity=0\n",
    "    )\n",
    "    model.fit(X_train_scaled, y_train_log)\n",
    "    \n",
    "    # 7. Evaluate on the test set\n",
    "    #    Predictions are in log space, so exponentiate back\n",
    "    y_pred_log = model.predict(X_test_scaled)\n",
    "    y_pred = np.expm1(y_pred_log)  # revert from log(EF) to EF\n",
    "    \n",
    "    # Convert y_test_log back for metrics\n",
    "    y_test = np.expm1(y_test_log)\n",
    "    \n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    # Adjusted R²\n",
    "    n = len(y_test)\n",
    "    p = X_test_scaled.shape[1]\n",
    "    if n > p + 1:\n",
    "        adjusted_r2 = 1 - (1 - r2) * (n - 1) / (n - p - 1)\n",
    "    else:\n",
    "        adjusted_r2 = float('nan')\n",
    "    \n",
    "    print(\"\\n=== XGBoost Model (Test Set) ===\")\n",
    "    print(\"Log-transform of Enhancement Factor + 256-bit fingerprints\")\n",
    "    print(\"n_estimators=100, max_depth=3, learning_rate=0.1\")\n",
    "    print(f\"MSE  : {mse:.4f}\")\n",
    "    print(f\"RMSE : {rmse:.4f}\")\n",
    "    print(f\"MAE  : {mae:.4f}\")\n",
    "    print(f\"R²   : {r2:.4f}\")\n",
    "    print(f\"Adjusted R²: {adjusted_r2:.4f}\")\n",
    "\n",
    "    # 8. (Optional) Retrain a final model on the entire dataset for new SMILES predictions\n",
    "    final_model = xgb.XGBRegressor(\n",
    "        n_estimators=100,\n",
    "        max_depth=3,\n",
    "        learning_rate=0.1,\n",
    "        random_state=42,\n",
    "        verbosity=0\n",
    "    )\n",
    "    # Scale the entire dataset\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    y_log_all = np.log1p(y)\n",
    "    final_model.fit(X_scaled, y_log_all)\n",
    "    \n",
    "    # 9. Interactive input for new SMILES outside the dataset\n",
    "    print(\"\\n=== Predict Enhancement Factor for a new SMILES ===\")\n",
    "    while True:\n",
    "        user_input = input(\"Enter a SMILES string (or type 'quit' to exit): \").strip()\n",
    "        if user_input.lower() == 'quit':\n",
    "            print(\"Exiting.\")\n",
    "            break\n",
    "        \n",
    "        new_fp = smiles_to_morgan_fp(user_input, radius=2, n_bits=256)\n",
    "        if new_fp is None:\n",
    "            print(\"Invalid SMILES entered. Please try again.\\n\")\n",
    "            continue\n",
    "        \n",
    "        # Scale the new fingerprint\n",
    "        new_fp_scaled = scaler.transform([new_fp])\n",
    "        # Predict in log space, then exponentiate\n",
    "        pred_log = final_model.predict(new_fp_scaled)[0]\n",
    "        pred_val = np.expm1(pred_log)\n",
    "        print(f\"Predicted Enhancement Factor: {pred_val:.4f}\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae70976",
   "metadata": {},
   "source": [
    "Now, we do an analysis for both validation and test set of model:\n",
    "    -\n",
    "    -\n",
    "    -\n",
    "    -\n",
    "    -\n",
    "    -\n",
    "    -\n",
    "    -\n",
    "    -\n",
    "    -\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f1009f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ID                         Name  \\\n",
      "0     1         Ibuprofen / HP-B-CD   \n",
      "1     2      Dexamethasone / B - CD   \n",
      "2     3         Piroxicam / HP-B-CD   \n",
      "3     4    Hydrocortisone / HP-B-CD   \n",
      "4     5       Albendazole / RM-B-CD   \n",
      "5     6            Valtarsan / B-CD   \n",
      "6     7        Nifedipine / HP-B-CD   \n",
      "7     8        Nimodipine / HP-B-CD   \n",
      "8     9     Carbamazepine / HP-B-CD   \n",
      "9    10      Itraconazole / HP-B-CD   \n",
      "10   11       Simvastatin / HP-B-CD   \n",
      "11   12          Curcumin / HP-B-CD   \n",
      "12   13  Indomethacin / HP- B - CD    \n",
      "13   14     Naproxen / HP - B - CD    \n",
      "14   15        Diclofenac / HP-B-CD   \n",
      "15   16   Resveratrol / HP-Gamma CD   \n",
      "16   17              Amphotericin B   \n",
      "17   18                 Budesonide    \n",
      "\n",
      "                                               Smiles MolWeight  LogP  \\\n",
      "0                          CC(C)Cc1ccc(cc1)C(C)C(=O)O   1606.29  3.84   \n",
      "1   CC(C)C1C(=O)CC2C(C1O)(CC(C1=CC(=O)C=C2C1(C)O)F...   1527.47  1.93   \n",
      "2                     CN1NS(=O)(=O)C2=C(O)C=CN=C2C1=O   1466.35   2.2   \n",
      "3              CC(=O)C1(CCC2C1(CCC1C(C2CC(C1O)O)O)C)O   1762.46  1.79   \n",
      "4                      CCCSC1=CC=C2C=C1N=C(C(=O)OC)N2   250.323  3.22   \n",
      "5   CC(C)C[C@H](C(=O)NCc1ccc(cc1)c1ccccc1C(=N)N=N)...    1570.5  3.68   \n",
      "6   CCOC(=O)C1=C(C)NC(=C(C1c1ccccc1[N+](=O)[O-])C)...      1846  2.49   \n",
      "7   COCCOC(=O)C1=C(C)NC(=C(C1c1ccccc1[N+](=O)[O-])...    418.44  3.41   \n",
      "8                        O=C1Nc2ccccc2C2=CC=CC=C2C1=O  1371.25    2.1   \n",
      "9   CC1=NC(C)=NC(OCC2CCC(OC3=NC(N4CCN(C)CC4)=NC(C4...    1840.6  5.48   \n",
      "10  CC(C)CC(CC=C)C(=O)OC[C@@H]1CC[C@@H](CC1)C(=O)O...   1553.55  4.51   \n",
      "11      COc1ccc(cc1)/C=C/C(=O)CC(=O)/C=C/c1ccc(cc1)OC   1503.36  3.62   \n",
      "12      COc1ccc2c(c1)n(c(c2C(=O)O)C(=O)c1ccc(cc1)Cl)C    1492.8  4.46   \n",
      "13                       COc1cc2ccc(C(C)C(=O)O)cc2cc1   1365.24  3.39   \n",
      "14                   OC(=O)Cc1cccc(c1)Nc1c(cccc1Cl)Cl   1431.15  4.75   \n",
      "15               OC1=CC=CC(/C=C/C2=CC(O)=CC(O)=C2)=C1  1728.247  2.57   \n",
      "16  CC(C)CCCC(C)CCCC(C)C(C)CCC/C=C\\C/C=C\\C/C=C\\C/C...         -     -   \n",
      "17  CC(=CC(=O)C12CCC(C)CC1CCC1C2CCC2OC(C)(C)CC1(C)...         -     -   \n",
      "\n",
      "       LogS (CD)  HPCalculated  Descriptors   \\\n",
      "0           -3.5           NaN           NaN   \n",
      "1    2.979852707           NaN           NaN   \n",
      "2    4.166237644           NaN           NaN   \n",
      "3    1.831145921           NaN           NaN   \n",
      "4      0.9213795           NaN           NaN   \n",
      "5    2.920818754           NaN           NaN   \n",
      "6    3.915983678           NaN           NaN   \n",
      "7    3.881270504           NaN           NaN   \n",
      "8   -2.331615782           NaN           NaN   \n",
      "9    4.833595654           NaN           NaN   \n",
      "10    3.93605273           NaN           NaN   \n",
      "11    4.08504452           NaN           NaN   \n",
      "12   4.229518954           NaN           NaN   \n",
      "13   1.754487332           NaN           NaN   \n",
      "14   4.803502637           NaN           NaN   \n",
      "15   3.196213127           NaN           NaN   \n",
      "16             -           NaN           NaN   \n",
      "17             -           NaN           NaN   \n",
      "\n",
      "    CD-complex solubility (mg/ml; unless stated otherwise)  \\\n",
      "0                                                 NaN        \n",
      "1                                            1.600000        \n",
      "2                                            0.100000        \n",
      "3                                           26.000000        \n",
      "4                                           30.000000        \n",
      "5                                            1.884600        \n",
      "6                                            0.224000        \n",
      "7                                            0.055000        \n",
      "8                                            6.390000        \n",
      "9                                            0.027000        \n",
      "10                                           0.180000        \n",
      "11                                           0.123600        \n",
      "12                                           0.088000        \n",
      "13                                          24.028224        \n",
      "14                                           0.022500        \n",
      "15                                           1.100000        \n",
      "16                                           0.150000        \n",
      "17                                           0.845000        \n",
      "\n",
      "    Intrinistic solubility (mg/ml; unless stated otherwise  Enhancement Factor  \n",
      "0                                                 NaN                30.000000  \n",
      "1                                            0.160000                10.000000  \n",
      "2                                            0.023000                 4.347826  \n",
      "3                                            0.280000                71.400000  \n",
      "4                                            0.010000               600.000000  \n",
      "5                                            0.003192               590.783000  \n",
      "6                                            0.005000                44.800000  \n",
      "7                                            0.002500                22.000000  \n",
      "8                                            0.140000                45.600000  \n",
      "9                                            0.000001             27000.000000  \n",
      "10                                           0.030000                 6.000000  \n",
      "11                                           0.000600               206.000000  \n",
      "12                                           0.008800                10.000000  \n",
      "13                                           0.115131               208.704900  \n",
      "14                                           0.003000                 7.500000  \n",
      "15                                           0.030000                36.666667  \n",
      "16                                           0.001000               150.000000  \n",
      "17                                           0.022000                38.409091  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[01:41:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:41:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:41:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:41:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:41:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:41:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:41:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:41:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:41:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:41:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:41:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:41:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:41:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:41:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:41:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:41:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:41:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:41:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:41:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:41:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:41:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:41:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:41:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:41:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:41:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:41:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:41:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:41:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:41:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:41:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:41:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:41:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:41:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:41:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:41:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:41:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:41:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:41:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:41:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:41:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:41:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:41:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:41:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:41:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:41:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:41:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:41:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:41:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:41:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:41:08] DEPRECATION WARNING: please use MorganGenerator\n",
      "[01:41:08] DEPRECATION WARNING: please use MorganGenerator\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== XGBoost Model (Validation Set) ===\n",
      "Log-transform of Enhancement Factor + 256-bit fingerprints\n",
      "n_estimators=100, max_depth=3, learning_rate=0.1\n",
      "MSE  : 26495375.0315\n",
      "RMSE : 5147.3658\n",
      "MAE  : 2174.1652\n",
      "R²   : 0.0781\n",
      "Adjusted R²: nan\n",
      "\n",
      "=== XGBoost Model (Test Set) ===\n",
      "Log-transform of Enhancement Factor + 256-bit fingerprints\n",
      "n_estimators=100, max_depth=3, learning_rate=0.1\n",
      "MSE  : 139428.4386\n",
      "RMSE : 373.4012\n",
      "MAE  : 161.2269\n",
      "R²   : -0.1383\n",
      "Adjusted R²: nan\n",
      "\n",
      "=== Predict Enhancement Factor for a new SMILES ===\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# RDKit for SMILES parsing\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "\n",
    "# For train-test split, metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# (Optional) Feature scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# XGBoost\n",
    "# pip install xgboost\n",
    "import xgboost as xgb\n",
    "\n",
    "def smiles_to_morgan_fp(smi, radius=2, n_bits=256):\n",
    "    \"\"\"\n",
    "    Convert a SMILES string to a Morgan fingerprint (ECFP) vector of length n_bits.\n",
    "    Returns a NumPy array of shape (n_bits,) or None if the SMILES is invalid.\n",
    "\n",
    "    We use 256 bits (instead of 1024) to reduce dimensionality for a small dataset.\n",
    "    \"\"\"\n",
    "    mol = Chem.MolFromSmiles(smi)\n",
    "    if mol is None:\n",
    "        return None\n",
    "    fp = AllChem.GetMorganFingerprintAsBitVect(mol, radius, nBits=n_bits)\n",
    "    arr = np.zeros((n_bits,), dtype=int)\n",
    "    for i in range(n_bits):\n",
    "        if fp.GetBit(i):\n",
    "            arr[i] = 1\n",
    "    return arr\n",
    "\n",
    "def main():\n",
    "    # 1. Load your CSV dataset\n",
    "    file_path = 'C:/Users/gagno/Downloads/molecule_dataset.csv'\n",
    "    df = pd.read_csv(file_path)\n",
    "    print(df.head(18))  # Quick peek at data\n",
    "    \n",
    "    # 2. Basic cleaning\n",
    "    df = df.replace([np.inf, -np.inf], np.nan)\n",
    "    df[\"Enhancement Factor\"] = pd.to_numeric(df[\"Enhancement Factor\"], errors='coerce')\n",
    "    df.dropna(subset=[\"Smiles\", \"Enhancement Factor\"], inplace=True)\n",
    "    \n",
    "    # 3. Convert SMILES to numeric features (256-bit Morgan fingerprints)\n",
    "    smiles_list = df[\"Smiles\"].astype(str).tolist()\n",
    "    enhancement_factors = df[\"Enhancement Factor\"].values\n",
    "    \n",
    "    X_fps = []\n",
    "    valid_y = []\n",
    "    for smi, val in zip(smiles_list, enhancement_factors):\n",
    "        fp = smiles_to_morgan_fp(smi, radius=2, n_bits=256)\n",
    "        if fp is not None:\n",
    "            X_fps.append(fp)\n",
    "            valid_y.append(val)\n",
    "    X = np.array(X_fps, dtype=float)\n",
    "    y = np.array(valid_y, dtype=float)\n",
    "    \n",
    "    # --- Log-Transform the Enhancement Factor ---\n",
    "    # If EF is always > 0, np.log1p helps handle large numeric ranges or outliers.\n",
    "    y_log = np.log1p(y)  # log(1 + EF)\n",
    "    \n",
    "    # 4. Three-way split: Train (60%), Validation (20%), Test (20%)\n",
    "    # First, split off 20% for test\n",
    "    X_temp, X_test, y_temp_log, y_test_log = train_test_split(\n",
    "        X, y_log, test_size=0.2, random_state=42\n",
    "    )\n",
    "    # Then, from the remaining 80%, split 25% (which is 0.25*80%=20% overall) for validation\n",
    "    X_train, X_val, y_train_log, y_val_log = train_test_split(\n",
    "        X_temp, y_temp_log, test_size=0.25, random_state=42\n",
    "    )\n",
    "    # Now we have: 60% train, 20% val, 20% test\n",
    "    \n",
    "    # 5. (Optional) Feature scaling\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_val_scaled   = scaler.transform(X_val)\n",
    "    X_test_scaled  = scaler.transform(X_test)\n",
    "    \n",
    "    # 6. Define and train an XGBoost regressor on the TRAIN set\n",
    "    model = xgb.XGBRegressor(\n",
    "        n_estimators=100,\n",
    "        max_depth=3,\n",
    "        learning_rate=0.1,\n",
    "        random_state=42,\n",
    "        verbosity=0\n",
    "    )\n",
    "    model.fit(X_train_scaled, y_train_log)\n",
    "    \n",
    "    # 7. Evaluate on the VALIDATION set\n",
    "    y_val_pred_log = model.predict(X_val_scaled)\n",
    "    y_val_pred = np.expm1(y_val_pred_log)  # revert from log(EF) to EF\n",
    "    y_val = np.expm1(y_val_log)\n",
    "    \n",
    "    mse_val  = mean_squared_error(y_val, y_val_pred)\n",
    "    rmse_val = np.sqrt(mse_val)\n",
    "    mae_val  = mean_absolute_error(y_val, y_val_pred)\n",
    "    r2_val   = r2_score(y_val, y_val_pred)\n",
    "    \n",
    "    # Adjusted R² on validation\n",
    "    n_val = len(y_val)\n",
    "    p_val = X_val_scaled.shape[1]\n",
    "    if n_val > p_val + 1:\n",
    "        adj_r2_val = 1 - (1 - r2_val) * (n_val - 1) / (n_val - p_val - 1)\n",
    "    else:\n",
    "        adj_r2_val = float('nan')\n",
    "    \n",
    "    print(\"\\n=== XGBoost Model (Validation Set) ===\")\n",
    "    print(\"Log-transform of Enhancement Factor + 256-bit fingerprints\")\n",
    "    print(\"n_estimators=100, max_depth=3, learning_rate=0.1\")\n",
    "    print(f\"MSE  : {mse_val:.4f}\")\n",
    "    print(f\"RMSE : {rmse_val:.4f}\")\n",
    "    print(f\"MAE  : {mae_val:.4f}\")\n",
    "    print(f\"R²   : {r2_val:.4f}\")\n",
    "    print(f\"Adjusted R²: {adj_r2_val:.4f}\")\n",
    "    \n",
    "    # 8. Evaluate on the TEST set\n",
    "    y_test_pred_log = model.predict(X_test_scaled)\n",
    "    y_test_pred = np.expm1(y_test_pred_log)\n",
    "    y_test_orig = np.expm1(y_test_log)\n",
    "    \n",
    "    mse_test  = mean_squared_error(y_test_orig, y_test_pred)\n",
    "    rmse_test = np.sqrt(mse_test)\n",
    "    mae_test  = mean_absolute_error(y_test_orig, y_test_pred)\n",
    "    r2_test   = r2_score(y_test_orig, y_test_pred)\n",
    "    \n",
    "    # Adjusted R² on test\n",
    "    n_test = len(y_test_orig)\n",
    "    p_test = X_test_scaled.shape[1]\n",
    "    if n_test > p_test + 1:\n",
    "        adj_r2_test = 1 - (1 - r2_test) * (n_test - 1) / (n_test - p_test - 1)\n",
    "    else:\n",
    "        adj_r2_test = float('nan')\n",
    "    \n",
    "    print(\"\\n=== XGBoost Model (Test Set) ===\")\n",
    "    print(\"Log-transform of Enhancement Factor + 256-bit fingerprints\")\n",
    "    print(\"n_estimators=100, max_depth=3, learning_rate=0.1\")\n",
    "    print(f\"MSE  : {mse_test:.4f}\")\n",
    "    print(f\"RMSE : {rmse_test:.4f}\")\n",
    "    print(f\"MAE  : {mae_test:.4f}\")\n",
    "    print(f\"R²   : {r2_test:.4f}\")\n",
    "    print(f\"Adjusted R²: {adj_r2_test:.4f}\")\n",
    "    \n",
    "    # 9. (Optional) If you want to re-train on the entire dataset for new SMILES predictions,\n",
    "    #    you can do so here. We'll skip that step in favor of the train/val/test approach.\n",
    "\n",
    "    # 10. Interactive input for new SMILES outside the dataset\n",
    "    print(\"\\n=== Predict Enhancement Factor for a new SMILES ===\")\n",
    "    while True:\n",
    "        user_input = input(\"Enter a SMILES string (or type 'quit' to exit): \").strip()\n",
    "        if user_input.lower() == 'quit':\n",
    "            print(\"Exiting.\")\n",
    "            break\n",
    "        \n",
    "        new_fp = smiles_to_morgan_fp(user_input, radius=2, n_bits=256)\n",
    "        if new_fp is None:\n",
    "            print(\"Invalid SMILES entered. Please try again.\\n\")\n",
    "            continue\n",
    "        \n",
    "        # Scale the new fingerprint\n",
    "        new_fp_scaled = scaler.transform([new_fp])\n",
    "        # Predict in log space, then exponentiate\n",
    "        pred_log = model.predict(new_fp_scaled)[0]\n",
    "        pred_val = np.expm1(pred_log)\n",
    "        print(f\"Predicted Enhancement Factor: {pred_val:.4f}\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bcae23a",
   "metadata": {},
   "source": [
    "Now, we produce the website version of this model:\n",
    "    =\n",
    "    =\n",
    "    =\n",
    "    =\n",
    "    =\n",
    "    =\n",
    "    =\n",
    "    =\n",
    "    =\n",
    "    =\n",
    "    =\n",
    "    =-\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00ce7b87",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Flask' object has no attribute 'before_first_request'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 44\u001b[0m\n\u001b[0;32m     39\u001b[0m app \u001b[38;5;241m=\u001b[39m Flask(\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m#############################################\u001b[39;00m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m# 3. Train XGBoost model on startup\u001b[39;00m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;66;03m#############################################\u001b[39;00m\n\u001b[1;32m---> 44\u001b[0m \u001b[38;5;129m@app\u001b[39m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbefore_first_request\u001b[49m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_model\u001b[39m():\n\u001b[0;32m     46\u001b[0m     \u001b[38;5;28;01mglobal\u001b[39;00m model, scaler, y_log_all\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;66;03m# 3a. Load dataset\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Flask' object has no attribute 'before_first_request'"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from flask import Flask, request, render_template_string\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "#############################################\n",
    "# 1. Utility function for SMILES -> fingerprint\n",
    "#############################################\n",
    "def smiles_to_morgan_fp(smi, radius=2, n_bits=256):\n",
    "    \"\"\"\n",
    "    Convert a SMILES string to a Morgan fingerprint (ECFP) vector of length n_bits.\n",
    "    Returns a NumPy array of shape (n_bits,) or None if SMILES is invalid.\n",
    "\n",
    "    Using 256 bits to reduce dimensionality.\n",
    "    \"\"\"\n",
    "    mol = Chem.MolFromSmiles(smi)\n",
    "    if mol is None:\n",
    "        return None\n",
    "    fp = AllChem.GetMorganFingerprintAsBitVect(mol, radius, nBits=n_bits)\n",
    "    arr = np.zeros((n_bits,), dtype=int)\n",
    "    for i in range(n_bits):\n",
    "        if fp.GetBit(i):\n",
    "            arr[i] = 1\n",
    "    return arr\n",
    "\n",
    "#############################################\n",
    "# 2. Create Flask app\n",
    "#############################################\n",
    "app = Flask(__name__)\n",
    "\n",
    "#############################################\n",
    "# 3. Train XGBoost model on startup\n",
    "#############################################\n",
    "@app.before_first_request\n",
    "def train_model():\n",
    "    global model, scaler, y_log_all\n",
    "    \n",
    "    # 3a. Load dataset\n",
    "    file_path = 'C:/Users/gagno/Downloads/molecule_dataset.csv'  # Adjust path if needed\n",
    "    df = pd.read_csv(file_path)\n",
    "    # Basic cleaning\n",
    "    df = df.replace([np.inf, -np.inf], np.nan)\n",
    "    df[\"Enhancement Factor\"] = pd.to_numeric(df[\"Enhancement Factor\"], errors='coerce')\n",
    "    df.dropna(subset=[\"Smiles\", \"Enhancement Factor\"], inplace=True)\n",
    "    \n",
    "    # 3b. Convert SMILES -> 256-bit fingerprints\n",
    "    smiles_list = df[\"Smiles\"].astype(str).tolist()\n",
    "    enhancement_factors = df[\"Enhancement Factor\"].values\n",
    "    \n",
    "    X_fps = []\n",
    "    valid_y = []\n",
    "    for smi, val in zip(smiles_list, enhancement_factors):\n",
    "        fp = smiles_to_morgan_fp(smi, radius=2, n_bits=256)\n",
    "        if fp is not None:\n",
    "            X_fps.append(fp)\n",
    "            valid_y.append(val)\n",
    "    X = np.array(X_fps, dtype=float)\n",
    "    y = np.array(valid_y, dtype=float)\n",
    "    \n",
    "    # 3c. Log-transform the target\n",
    "    y_log_all = np.log1p(y)\n",
    "    \n",
    "    # 3d. Scale features\n",
    "    global X_scaled\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    # 3e. Train XGBoost\n",
    "    global final_model\n",
    "    final_model = xgb.XGBRegressor(\n",
    "        n_estimators=100,\n",
    "        max_depth=3,\n",
    "        learning_rate=0.1,\n",
    "        random_state=42,\n",
    "        verbosity=0\n",
    "    )\n",
    "    final_model.fit(X_scaled, y_log_all)\n",
    "\n",
    "#############################################\n",
    "# 4. Define a simple HTML template\n",
    "#############################################\n",
    "HTML_TEMPLATE = \"\"\"\n",
    "<!DOCTYPE html>\n",
    "<html lang=\"en\">\n",
    "<head>\n",
    "  <meta charset=\"UTF-8\">\n",
    "  <title>Enhancement Factor Predictor</title>\n",
    "  <!-- Basic Bootstrap for a decorated, interactive page -->\n",
    "  <link rel=\"stylesheet\" href=\"https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css\">\n",
    "  <style>\n",
    "    body {\n",
    "      background: linear-gradient(to bottom right, #e3f2fd, #fff);\n",
    "      font-family: 'Helvetica Neue', Arial, sans-serif;\n",
    "      margin: 0; padding: 0;\n",
    "    }\n",
    "    .container {\n",
    "      margin-top: 50px;\n",
    "      max-width: 600px;\n",
    "      background: #fefefe;\n",
    "      border-radius: 10px;\n",
    "      padding: 30px;\n",
    "      box-shadow: 0 0 15px rgba(0,0,0,0.1);\n",
    "    }\n",
    "    h1 {\n",
    "      text-align: center;\n",
    "      margin-bottom: 30px;\n",
    "    }\n",
    "    .footer {\n",
    "      margin-top: 30px;\n",
    "      text-align: center;\n",
    "      color: #777;\n",
    "      font-size: 0.9em;\n",
    "    }\n",
    "  </style>\n",
    "</head>\n",
    "<body>\n",
    "  <div class=\"container\">\n",
    "    <h1>Enhancement Factor Predictor</h1>\n",
    "    <p class=\"text-secondary\">\n",
    "      Enter a SMILES string below. Our XGBoost model (log-transform + 256-bit Morgan fingerprints) \n",
    "      will predict the Enhancement Factor for your molecule.\n",
    "    </p>\n",
    "    <form method=\"POST\" action=\"/predict\">\n",
    "      <div class=\"mb-3\">\n",
    "        <label for=\"smiles\" class=\"form-label\">SMILES:</label>\n",
    "        <input type=\"text\" class=\"form-control\" id=\"smiles\" name=\"smiles\" placeholder=\"e.g. CC(=O)Oc1ccccc1C(=O)O\" required>\n",
    "      </div>\n",
    "      <button type=\"submit\" class=\"btn btn-primary\">Predict</button>\n",
    "    </form>\n",
    "    {% if prediction %}\n",
    "    <hr>\n",
    "    <div class=\"alert alert-info mt-4\" role=\"alert\">\n",
    "      <h4>Prediction</h4>\n",
    "      <p><strong>SMILES:</strong> {{ smiles_input }}</p>\n",
    "      <p><strong>Predicted Enhancement Factor:</strong> {{ prediction }}</p>\n",
    "    </div>\n",
    "    {% endif %}\n",
    "  </div>\n",
    "  <div class=\"footer\">\n",
    "    <p>Powered by XGBoost | <a href=\"https://github.com/\">View on GitHub</a></p>\n",
    "  </div>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "#############################################\n",
    "# 5. Routes\n",
    "#############################################\n",
    "@app.route(\"/\", methods=[\"GET\"])\n",
    "def index():\n",
    "    # Just render the form\n",
    "    return render_template_string(HTML_TEMPLATE)\n",
    "\n",
    "@app.route(\"/predict\", methods=[\"POST\"])\n",
    "def predict():\n",
    "    # Retrieve the SMILES string from the form\n",
    "    smiles_input = request.form.get(\"smiles\", \"\").strip()\n",
    "    \n",
    "    # Convert to fingerprint\n",
    "    fp = smiles_to_morgan_fp(smiles_input, radius=2, n_bits=256)\n",
    "    if fp is None:\n",
    "        # Invalid SMILES\n",
    "        prediction = \"Invalid SMILES entered. Please try again.\"\n",
    "        return render_template_string(HTML_TEMPLATE, prediction=prediction, smiles_input=smiles_input)\n",
    "    \n",
    "    # Scale\n",
    "    global scaler, final_model, y_log_all\n",
    "    fp_scaled = scaler.transform([fp])\n",
    "    \n",
    "    # Predict in log space, then exponentiate\n",
    "    pred_log = final_model.predict(fp_scaled)[0]\n",
    "    pred_val = np.expm1(pred_log)\n",
    "    prediction = f\"{pred_val:.4f}\"\n",
    "    \n",
    "    return render_template_string(HTML_TEMPLATE, prediction=prediction, smiles_input=smiles_input)\n",
    "\n",
    "#############################################\n",
    "# 6. Run Flask app\n",
    "#############################################\n",
    "if __name__ == \"__main__\":\n",
    "    app.run(debug=True, use_reloader=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d7f417",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
